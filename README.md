# ğŸ–ï¸ Sign_Language_Detection
This project detects and recognizes sign language gestures in real-time using Computer Vision and a pre-trained Machine Learning model. The goal is to help bridge communication between sign language users and non-users.

## ğŸ“Œ Features

> Live webcam streaming for gesture detection

> Hand detection using cvzone.HandTrackingModule

> Gesture classification with a custom-trained Keras model (keras_model.h5)

> Gesture list panel with images for reference

> Interactive Streamlit web app with user-friendly UI

## ğŸš€ Tech Stack

>Python

>Streamlit â€“ for web interface

>OpenCV (cv2) â€“ for video processing

>cvzone â€“ for hand tracking

>TensorFlow/Keras â€“ for model training and inference

>NumPy â€“ for array operations

## ğŸ“‚Project Structure
Sign-Language-Detection/
â”‚â”€â”€ Model/
â”‚   â”œâ”€â”€ keras_model.h5        # Pre-trained model
â”‚   â”œâ”€â”€ labels.txt            # Gesture labels
â”‚
â”‚â”€â”€ images/                   # Gesture reference images
â”‚   â”œâ”€â”€ hello.jpg
â”‚   â”œâ”€â”€ yes.jpg
â”‚   â”œâ”€â”€ no.jpg
â”‚   â””â”€â”€ ...
â”‚
â”‚â”€â”€ app.py                    # Main Streamlit app
â”‚â”€â”€ requirements.txt          # Dependencies
â”‚â”€â”€ README.md                 # Project documentation

## âš™ï¸ Installation

### Clone this repository:

git clone https://github.com/your-username/sign-language-detection.git
cd sign-language-detection


### Install dependencies:

pip install -r requirements.txt


### Run the Streamlit app:

streamlit run app.py

## ğŸ¯ Usage

>Launch the app.

>The gesture list panel on the left shows available gestures with sample images.

>Allow webcam access.

>Perform any gesture in front of the camera.

>The app will display the prediction in real-time.

## ğŸ–¼ï¸ Example Output

Gesture: "Yes" â†’ Prediction displayed on screen.

Gesture: "Help" â†’ Model recognizes and highlights it.
